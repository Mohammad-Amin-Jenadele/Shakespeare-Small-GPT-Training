{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import torch\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the Shakespeare.txt file\n",
    "with open('Shakespeare.txt', 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text : 1115394\n",
      "\n",
      "First 1000 characters of the text : \n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the text : {len(text)}\\n')\n",
    "print(f'First 1000 characters of the text : \\n{text[:1000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: [0, 3, 4, 5, 6, 41, 42, 43, 44, 12, 45, 46, 47, 48, 49, 50, 51, 52, 16, 6, 53, 54, 55, 28, 2]\n",
      "Original Text: First Citizen : \n",
      " Let us kill him , and we'll have corn at our own price . \n",
      " Is't a verdict ?\n"
     ]
    }
   ],
   "source": [
    "# Making a dictionary for the text\n",
    "nltk.download('punkt')\n",
    "repetition_threshold = 1  # Set your desired repetition threshold\n",
    "tokenizer = utils.TextTokenizer(repetition_threshold)\n",
    "tokenizer.process_text(text)\n",
    "# Example text and tokenized text\n",
    "example_text = \"First Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\"\n",
    "tokenized_text = tokenizer.text_to_tokens(example_text)\n",
    "print(\"Tokenized Text:\", tokenized_text)\n",
    "\n",
    "# Convert tokenized text back to original text\n",
    "original_text = tokenizer.tokens_to_text(tokenized_text)\n",
    "print(\"Original Text:\", original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([290403])\n",
      "tensor([  0,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,\n",
      "         16,   6,   6,  17,   5,   6,  18,  12,  15,  16,   6,   6,   3,   4,\n",
      "          5,   6,  19,  20,  21,  22,  23,  24,  25,  26,  24,  27,  28,   6,\n",
      "          6,  17,   5,   6,  29,  16,  22,  16,   6,   6,   3,   4,   5,   6,\n",
      "          3,  12,  30,  31,  32,  33,  34,  35,  36,  24,  37,  38,  16,   6,\n",
      "          6,  17,   5,   6,  39,  40,  12,   8,  40,  16,   6,   6,   3,   4,\n",
      "          5,   6,  41,  42,  43,  44,  12,  45,  46,  47,  48,  49,  50,  51,\n",
      "         52,  16,   6,  53,  54,  55,  28,   6,   6,  17,   5,   6,  56,  57,\n",
      "         58,  59,  60,  61,  62,  63,  64,   5,  65,  12,  65,  66,   6,   6,\n",
      "         67,   4,   5,   6,  68,  69,  12,  70,  71,  16,   6,   6,   3,   4,\n",
      "          5,   6,  39,  20,  72,  73,  71,  12,  37,  74,  70,  16,   6,  75,\n",
      "         76,  77,  78,  79,  80,  42,   5,  81,  82,   6,  79,  83,  42,  84,\n",
      "         37,  85,  12,  86,  62,  87,   6,  88,  12,   8,  89,  90,  82,   1,\n",
      "         42,   1,  60,   6,  84,  82,  91,   8,  20,  92,  93,   5,  37,  94,\n",
      "         95,   6,   1,  42,  12,  37,  96,  97,  50,  98,  12,  34,  99, 100,\n",
      "          6,   1,  24,   1, 101, 102,  60,  50,   6, 103,  34,  54, 104,  24,\n",
      "        105,  41,  42, 106, 107, 108,   6,  50, 109,  12, 110,   8, 111,   1,\n",
      "          5, 112,  37, 113,  31, 114,   6,  15, 107, 115, 116, 112, 117,  12,\n",
      "        118, 115, 119, 112, 106,  16,   6,   6,  67,   4,   5,   6, 120,  30,\n",
      "          9, 121, 122,  32,  33,  28,   6,   6,  17,   5,   6, 123,  44, 124,\n",
      "          5, 125,  54, 126, 127,  24,  37,   1,  16,   6,   6,  67,   4,   5,\n",
      "          6, 128,  30, 129, 130, 131, 132,  64, 112, 133, 134,  28,   6,   6,\n",
      "          3,   4,   5,   6, 135, 136,  60,  45, 137,  63, 138,  24, 139,  44,\n",
      "         70,   6, 140, 141,  12,  84,  95, 131, 142, 143, 108, 144, 145,  16,\n",
      "          6,   6,  67,   4,   5,   6, 146,  12,  84,  15, 118,   1,  16,   6,\n",
      "          6,   3,   4,   5,   6, 114, 147, 148,  30,  12, 129, 131, 149,  64,\n",
      "        150,  12, 131, 151,   6,  62,  24,  95, 152,   5, 153, 154,   1, 155,\n",
      "        156,  63,   6, 138,  24, 147,  62, 157, 112, 133, 134, 131, 151,  62,\n",
      "         24,   6, 158, 133, 159,  45,  24,  63, 160, 145,  60, 161, 131,   6,\n",
      "         34,  12, 162, 163,  37,   1,  97, 133, 164,  16,   6,   6,  67,   4,\n",
      "          5,   6,  75, 131, 165, 166, 115, 133, 167,  12,  30, 168,  54,   6,\n",
      "        169, 115,  44,  16,  19, 170, 115, 171, 172, 147, 131,  34,   1,  16,\n",
      "          6,   6,   3,   4,   5,   6, 173, 114, 170, 118,  12, 114, 174, 118,\n",
      "         63, 175,  97, 176,  60,   6, 131, 149, 177,  12, 108, 178,  12,  24,\n",
      "        179, 115, 180,  16,   6,  75, 181,  20, 182,  28, 183, 184, 185, 186,\n",
      "         37, 187,   6,  34, 188,   5, 189, 190,   8, 191, 192,  28,  24,  37,\n",
      "        193,  66,   6,   6,  17,   5,   6, 194,  12, 195,  16,   6,   6,   3,\n",
      "          4,   5,   6, 196,  66, 197, 198, 192,  28,   6,   6,  67,   4,   5,\n",
      "          6, 199, 200,   1,  60, 201,  95, 149, 202, 203,   6,  37,  38,  16,\n",
      "          6,   6,   3,   4,   5,   6, 204, 201, 205, 206,   5,  79,  21,  37,\n",
      "        207,  87, 208,  66,   6,   6, 209,   5,   6,  75,   1,  12, 210, 211,\n",
      "         12, 115, 212,  28, 213, 214,  30,   6, 215, 216,  45, 217,  28, 183,\n",
      "        218,  28,  15,  12, 114, 219,  30,  16,   6,   6,   3,   4,   5,   6,\n",
      "        220, 221,  34, 118, 222,  24,  37, 223,  60,  82,  47,   6, 224,   1,\n",
      "        107, 225, 129,   8, 226,  24, 227,  12,   6, 161, 228,  46, 229, 230,\n",
      "        115, 231,  16, 232, 147,  73,   6, 233,  47, 234, 235,   5,  82, 236,\n",
      "         31,   8,   6,  47, 234, 237,  92,  16,   6,   6, 209,   5,   6, 238,\n",
      "         12, 239,  12, 210,  70, 240,  12, 241, 205, 242,  12,   6, 243,  30,\n",
      "        244, 245,  28,   6,   6,   3,   4,   5,   6,  39, 165,  12, 246,  12,\n",
      "          8,  20, 247, 248,  16,   6,   6, 209,   5,   6, 114, 249,  30,  12,\n",
      "        240,  12, 250, 251, 252,   6, 253,  37,  74,  97,  30,  16, 254, 255,\n",
      "        256,  12,   6, 257, 258, 115, 107, 259,  12,  30, 260,  99, 136,   6,\n",
      "        261,  49,  37, 262, 108, 255, 263,  99, 264, 105,   6, 123,  37, 265,\n",
      "        266,  12, 267, 268, 269,  78,   6, 183, 172,  62, 270,  12, 271, 272,\n",
      "        273, 274,   6, 275,  57, 234, 276, 277,  26, 156, 278,   6, 279, 115,\n",
      "        255, 280,  16, 254,  37, 259,  12,   6, 183, 113,  12, 118,  37,  74,\n",
      "         12, 281,  62,  12,  45,   6, 257, 282,  24, 105,  12, 118, 237,  12,\n",
      "        170, 166,  16, 283,  12,   6,  19,  20, 284, 285, 286,   6, 287, 213,\n",
      "         57,   1,  30,  12,  45,  30, 288,   6, 183, 289, 186,  37, 266,  12,\n",
      "        197, 252, 112,  30, 290, 291,  12,   6, 292,  30, 293, 105,  99, 294,\n",
      "         16,   6,   6,   3,   4,   5,   6, 295, 112,  42,  66, 296,  12, 297,\n",
      "         66, 232, 298,   1, 112,  42,   6, 299,   5, 300,  42,  24,  27,  12,\n",
      "         45, 101, 301, 302,   6,   1, 108, 303,  60, 281,   1, 112,   1,  12,\n",
      "         24,   6, 304,   1,  60, 305, 306,  10,  88, 307,   6,   1, 122,  37,\n",
      "        308,  12,  45, 309,  57,   6, 310, 311, 306,  12,  24, 312, 313,  45,\n",
      "          1,   6,  37,  73,  16, 173,  37, 314, 315,  42, 118, 313,  12,  82,\n",
      "        269,  60,  45,   6, 316,  21,  37, 317,  82, 318,  42,  16,   6,   6,\n",
      "        209,   5,   6, 319,  30, 170,   6, 320, 245, 321, 322,  12,   6, 323,\n",
      "         63, 324,  97, 325,  16, 114, 236, 249,  30,   6, 326, 327, 328,   5,\n",
      "         62, 260,  63,  30,  47, 329,  62,  60,   6, 330,  12, 331,  62, 332,\n",
      "        210, 333,  12, 114, 269, 334,   6, 335, 336, 337,  54, 338,  57,  16,\n",
      "          6,   6,   3,   4,   5,   6])\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the entire Shakespeare text\n",
    "data = torch.tensor(tokenizer.text_to_tokens(text))\n",
    "print(data.shape)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the validation and training data\n",
    "n = int(0.9 * len(data))\n",
    "training_set = data[:n]\n",
    "validation_set = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below , we are determining the `block_size` which is the size of each training data . But each example , has `block_size` examples within iy self . In the cell below , it is shown by an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an example of a data : tensor([0, 3, 4, 5, 6, 7, 8, 9])\n",
      "input : tensor([0]) , target : 3\n",
      "input : tensor([0, 3]) , target : 4\n",
      "input : tensor([0, 3, 4]) , target : 5\n",
      "input : tensor([0, 3, 4, 5]) , target : 6\n",
      "input : tensor([0, 3, 4, 5, 6]) , target : 7\n",
      "input : tensor([0, 3, 4, 5, 6, 7]) , target : 8\n",
      "input : tensor([0, 3, 4, 5, 6, 7, 8]) , target : 9\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "x = training_set[:block_size]\n",
    "print(f'an example of a data : {x}')\n",
    "for t in range(1,block_size):\n",
    "    context = x[:t]\n",
    "    target = x[t]\n",
    "    print(f'input : {context} , target : {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs =\n",
      "tensor([[  14,  192,   12,    6,  676,    1, 1081,  615],\n",
      "        [  99,    1,   99,  689,   12,    6, 6327,   99],\n",
      "        [6028,    5,    6,    1,  112, 5502,   16,    6],\n",
      "        [ 450,  115,  317,   16,    6,    6, 5995,    5]])\n",
      "outputs =\n",
      "tensor([[ 192,   12,    6,  676,    1, 1081,  615,   12],\n",
      "        [   1,   99,  689,   12,    6, 6327,   99, 2166],\n",
      "        [   5,    6,    1,  112, 5502,   16,    6,    6],\n",
      "        [ 115,  317,   16,    6,    6, 5995,    5,    6]])\n",
      "------------------------ EXAMPLE ------------------------\n",
      "input : [14] , target : 192\n",
      "input : [14, 192] , target : 12\n",
      "input : [14, 192, 12] , target : 6\n",
      "input : [14, 192, 12, 6] , target : 676\n",
      "input : [14, 192, 12, 6, 676] , target : 1\n",
      "input : [14, 192, 12, 6, 676, 1] , target : 1081\n",
      "input : [14, 192, 12, 6, 676, 1, 1081] , target : 615\n",
      "input : [14, 192, 12, 6, 676, 1, 1081, 615] , target : 12\n",
      "input : [99] , target : 1\n",
      "input : [99, 1] , target : 99\n",
      "input : [99, 1, 99] , target : 689\n",
      "input : [99, 1, 99, 689] , target : 12\n",
      "input : [99, 1, 99, 689, 12] , target : 6\n",
      "input : [99, 1, 99, 689, 12, 6] , target : 6327\n",
      "input : [99, 1, 99, 689, 12, 6, 6327] , target : 99\n",
      "input : [99, 1, 99, 689, 12, 6, 6327, 99] , target : 2166\n",
      "input : [6028] , target : 5\n",
      "input : [6028, 5] , target : 6\n",
      "input : [6028, 5, 6] , target : 1\n",
      "input : [6028, 5, 6, 1] , target : 112\n",
      "input : [6028, 5, 6, 1, 112] , target : 5502\n",
      "input : [6028, 5, 6, 1, 112, 5502] , target : 16\n",
      "input : [6028, 5, 6, 1, 112, 5502, 16] , target : 6\n",
      "input : [6028, 5, 6, 1, 112, 5502, 16, 6] , target : 6\n",
      "input : [450] , target : 115\n",
      "input : [450, 115] , target : 317\n",
      "input : [450, 115, 317] , target : 16\n",
      "input : [450, 115, 317, 16] , target : 6\n",
      "input : [450, 115, 317, 16, 6] , target : 6\n",
      "input : [450, 115, 317, 16, 6, 6] , target : 5995\n",
      "input : [450, 115, 317, 16, 6, 6, 5995] , target : 5\n",
      "input : [450, 115, 317, 16, 6, 6, 5995, 5] , target : 6\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 \n",
    "block_size = 8  # Number of maximum context length\n",
    "\n",
    "def get_batch(dataset):\n",
    "    data = training_set if dataset == 'train' else  validation_set\n",
    "    ix = torch.randint(len(data) - block_size , size = (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x , y\n",
    "\n",
    "\n",
    "x_b , y_b = get_batch('train')\n",
    "print(f'inputs =\\n{x_b}')\n",
    "print(f'outputs =\\n{y_b}')\n",
    "\n",
    "# An Example\n",
    "print('------------------------ EXAMPLE ------------------------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = x_b[b , : t+1]\n",
    "        target = y_b[b  , t]\n",
    "        print(f'input : {context.tolist()} , target : {target}')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
